{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Binary Classification + SageMaker Model Monitor v2\n",
    "\n",
    "このノートブックでは、LightGBMを使った2値分類モデルをSageMakerにデプロイし、Model Monitorで監視する完全なワークフローを実装します。\n",
    "\n",
    "## 主な改良点\n",
    "- より堅牢なエラーハンドリング\n",
    "- 詳細なログ出力\n",
    "- 設定可能なパラメータ\n",
    "- 自動クリーンアップ機能\n",
    "- 包括的なテスト機能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境設定とライブラリインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインストール\n",
    "!pip install lightgbm>=3.3.0 -q\n",
    "!pip install sagemaker>=2.100.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import tarfile\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# SageMaker関連\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.model_monitor import (\n",
    "    DataCaptureConfig,\n",
    "    DefaultModelMonitor,\n",
    "    DatasetFormat,\n",
    "    EndpointInput,\n",
    "    CronExpressionGenerator\n",
    ")\n",
    "\n",
    "# 警告を抑制\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ログ設定\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 初期設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker設定\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = session.boto_region_name\n",
    "bucket = session.default_bucket()\n",
    "s3_client = boto3.client('s3')\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Bucket: {bucket}\")\n",
    "print(f\"Role: {role}\")\n",
    "\n",
    "# 設定パラメータ\n",
    "config = {\n",
    "    # データ生成設定\n",
    "    'n_samples': 3000,\n",
    "    'n_features': 25,\n",
    "    'n_informative': 20,\n",
    "    'n_redundant': 5,\n",
    "    'n_clusters_per_class': 2,\n",
    "    'test_size': 0.2,\n",
    "    'val_size': 0.1,\n",
    "    'random_state': 42,\n",
    "    \n",
    "    # LightGBMパラメータ\n",
    "    'lgb_params': {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42\n",
    "    },\n",
    "    'num_boost_round': 100,\n",
    "    'early_stopping_rounds': 10,\n",
    "    \n",
    "    # デプロイ設定\n",
    "    'model_name': f'lightgbm-binary-v2-{int(time.time())}',\n",
    "    'endpoint_name': f'lightgbm-binary-endpoint-v2-{int(time.time())}',\n",
    "    'instance_type': 'ml.m5.large',\n",
    "    'initial_instance_count': 1,\n",
    "    \n",
    "    # Data Capture設定\n",
    "    'enable_data_capture': True,\n",
    "    'sampling_percentage': 100,\n",
    "    'capture_modes': ['Input', 'Output'],\n",
    "    \n",
    "    # Model Monitor設定\n",
    "    'monitor_schedule_name': f'lightgbm-monitor-v2-{int(time.time())}',\n",
    "    'monitoring_instance_type': 'ml.m5.xlarge',\n",
    "    'monitoring_cron': 'cron(0/10 * ? * * *)',  # 10分毎\n",
    "    'max_runtime_seconds': 3600,\n",
    "    \n",
    "    # S3設定\n",
    "    's3_prefix': 'lightgbm-binary-classification-v2',\n",
    "    'model_artifacts_path': 'model',\n",
    "    'baseline_path': 'baseline',\n",
    "    'monitoring_reports_path': 'monitoring-reports',\n",
    "    'data_capture_path': 'datacapture'\n",
    "}\n",
    "\n",
    "# S3パス設定\n",
    "s3_model_path = f's3://{bucket}/{config[\"s3_prefix\"]}/{config[\"model_artifacts_path\"]}'\n",
    "s3_baseline_path = f's3://{bucket}/{config[\"s3_prefix\"]}/{config[\"baseline_path\"]}'\n",
    "s3_monitoring_path = f's3://{bucket}/{config[\"s3_prefix\"]}/{config[\"monitoring_reports_path\"]}'\n",
    "s3_data_capture_path = f's3://{bucket}/{config[\"s3_prefix\"]}/{config[\"data_capture_path\"]}'\n",
    "\n",
    "print(f\"\\n📁 S3 Paths:\")\n",
    "print(f\"Model: {s3_model_path}\")\n",
    "print(f\"Baseline: {s3_baseline_path}\")\n",
    "print(f\"Monitoring: {s3_monitoring_path}\")\n",
    "print(f\"Data Capture: {s3_data_capture_path}\")\n",
    "\n",
    "print(f\"\\n🚀 Configuration:\")\n",
    "print(f\"Endpoint: {config['endpoint_name']}\")\n",
    "print(f\"Monitor: {config['monitor_schedule_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. データ生成と前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    \"\"\"\n",
    "    2値分類用のサンプルデータを生成\n",
    "    \"\"\"\n",
    "    print(\"📊 Generating sample data for binary classification...\")\n",
    "    \n",
    "    # データ生成\n",
    "    X, y = make_classification(\n",
    "        n_samples=config['n_samples'],\n",
    "        n_features=config['n_features'],\n",
    "        n_informative=config['n_informative'],\n",
    "        n_redundant=config['n_redundant'],\n",
    "        n_clusters_per_class=config['n_clusters_per_class'],\n",
    "        random_state=config['random_state']\n",
    "    )\n",
    "    \n",
    "    # データ分割\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=config['test_size'], \n",
    "        random_state=config['random_state'], stratify=y\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=config['val_size']/(1-config['test_size']), \n",
    "        random_state=config['random_state'], stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Data generated successfully:\")\n",
    "    print(f\"  Train: {X_train.shape[0]} samples\")\n",
    "    print(f\"  Validation: {X_val.shape[0]} samples\")\n",
    "    print(f\"  Test: {X_test.shape[0]} samples\")\n",
    "    print(f\"  Features: {X_train.shape[1]}\")\n",
    "    print(f\"  Class distribution (train): {np.bincount(y_train)}\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# データ生成実行\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LightGBMモデル訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \"\"\"\n",
    "    LightGBMモデルを訓練\n",
    "    \"\"\"\n",
    "    print(\"🤖 Training LightGBM model...\")\n",
    "    \n",
    "    # LightGBMデータセット作成\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    # モデル訓練\n",
    "    model = lgb.train(\n",
    "        config['lgb_params'],\n",
    "        train_data,\n",
    "        num_boost_round=config['num_boost_round'],\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'val'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(config['early_stopping_rounds']),\n",
    "            lgb.log_evaluation(period=10)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Model training completed successfully\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"\n",
    "    モデルの性能を評価\n",
    "    \"\"\"\n",
    "    # 予測\n",
    "    y_pred_proba = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # メトリクス計算\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(\"📈 Model Performance Metrics:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "# モデル訓練実行\n",
    "model = train_model()\n",
    "\n",
    "# モデル評価\n",
    "metrics = evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. モデル保存と推論スクリプト作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    \"\"\"\n",
    "    モデルをS3に保存\n",
    "    \"\"\"\n",
    "    print(\"💾 Saving model artifacts...\")\n",
    "    \n",
    "    # ローカルディレクトリ作成\n",
    "    os.makedirs('model', exist_ok=True)\n",
    "    \n",
    "    # モデルをpickle形式で保存（LightGBMのunordered_map::atエラー回避）\n",
    "    with open('model/model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    # テキスト形式でも保存（デバッグ用）\n",
    "    model.save_model('model/model.txt')\n",
    "    \n",
    "    # tar.gz形式でアーカイブ\n",
    "    with tarfile.open('model.tar.gz', 'w:gz') as tar:\n",
    "        tar.add('model', arcname='.')\n",
    "    \n",
    "    # S3にアップロード\n",
    "    model_uri = f'{s3_model_path}/model.tar.gz'\n",
    "    session.upload_data('model.tar.gz', bucket=bucket, \n",
    "                        key_prefix=f'{config[\"s3_prefix\"]}/{config[\"model_artifacts_path\"]}')\n",
    "    \n",
    "    print(f\"✅ Model saved to: {model_uri}\")\n",
    "    return model_uri\n",
    "\n",
    "def create_inference_script():\n",
    "    \"\"\"\n",
    "    推論スクリプトを作成\n",
    "    \"\"\"\n",
    "    print(\"📝 Creating inference script...\")\n",
    "    \n",
    "    os.makedirs('source', exist_ok=True)\n",
    "    \n",
    "    inference_code = '''import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"モデルをロード\"\"\"\n",
    "    try:\n",
    "        with open(f\"{model_dir}/model.pkl\", \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "        logger.info(\"Model loaded successfully from pickle file\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"入力データを処理\"\"\"\n",
    "    if request_content_type == \"application/json\":\n",
    "        try:\n",
    "            input_data = json.loads(request_body)\n",
    "            \n",
    "            # 単一サンプルの場合\n",
    "            if \"instances\" in input_data:\n",
    "                data = np.array(input_data[\"instances\"])\n",
    "            elif isinstance(input_data, list):\n",
    "                data = np.array(input_data)\n",
    "            else:\n",
    "                data = np.array([input_data])\n",
    "                \n",
    "            logger.info(f\"Input data shape: {data.shape}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing input: {str(e)}\")\n",
    "            raise ValueError(f\"Invalid input format: {str(e)}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"予測を実行\"\"\"\n",
    "    try:\n",
    "        # 確率予測\n",
    "        probabilities = model.predict(input_data, num_iteration=model.best_iteration)\n",
    "        \n",
    "        # 2値分類の場合、クラス1の確率を返す\n",
    "        if len(probabilities.shape) == 1:\n",
    "            predictions = probabilities\n",
    "        else:\n",
    "            predictions = probabilities[:, 1]\n",
    "            \n",
    "        # 閾値0.5でクラス予測\n",
    "        classes = (predictions > 0.5).astype(int)\n",
    "        \n",
    "        logger.info(f\"Predictions generated for {len(predictions)} samples\")\n",
    "        \n",
    "        return {\n",
    "            \"predictions\": classes.tolist(),\n",
    "            \"probabilities\": predictions.tolist()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during prediction: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    \"\"\"出力を処理\"\"\"\n",
    "    if content_type == \"application/json\":\n",
    "        return json.dumps(prediction)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {content_type}\")\n",
    "'''\n",
    "    \n",
    "    with open('source/inference.py', 'w') as f:\n",
    "        f.write(inference_code)\n",
    "    \n",
    "    # requirements.txt作成\n",
    "    requirements = '''lightgbm>=3.3.0\n",
    "numpy>=1.21.0\n",
    "scikit-learn>=1.0.0\n",
    "'''\n",
    "    \n",
    "    with open('source/requirements.txt', 'w') as f:\n",
    "        f.write(requirements)\n",
    "    \n",
    "    print(\"✅ Inference script created successfully\")\n",
    "\n",
    "# モデル保存と推論スクリプト作成\n",
    "model_uri = save_model(model)\n",
    "create_inference_script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
