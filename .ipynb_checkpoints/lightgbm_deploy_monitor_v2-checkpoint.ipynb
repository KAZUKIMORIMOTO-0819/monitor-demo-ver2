{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM v2 - ãƒ‡ãƒ—ãƒ­ã‚¤ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ `lightgbm_complete_v2.ipynb` ã®ç¶šãã§ã™ã€‚\n",
    "ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã€Model Monitorè¨­å®šã€ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SageMakerã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ—ãƒ­ã‚¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model():\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ã‚’SageMakerã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«ãƒ‡ãƒ—ãƒ­ã‚¤\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ Deploying model to SageMaker endpoint...\")\n",
    "    \n",
    "    # LightGBMç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠã‚¤ãƒ¡ãƒ¼ã‚¸å–å¾—\n",
    "    image_uri = retrieve(\n",
    "        framework='sklearn',\n",
    "        region=region,\n",
    "        version='1.0-1',\n",
    "        py_version='py3',\n",
    "        instance_type=config['instance_type']\n",
    "    )\n",
    "    \n",
    "    # Data Captureè¨­å®š\n",
    "    data_capture_config = None\n",
    "    if config['enable_data_capture']:\n",
    "        data_capture_config = DataCaptureConfig(\n",
    "            enable_capture=True,\n",
    "            sampling_percentage=config['sampling_percentage'],\n",
    "            destination_s3_uri=s3_data_capture_path,\n",
    "            capture_options=config['capture_modes']\n",
    "        )\n",
    "        print(f\"ğŸ“Š Data capture enabled: {s3_data_capture_path}\")\n",
    "    \n",
    "    # SageMakerãƒ¢ãƒ‡ãƒ«ä½œæˆ\n",
    "    model = Model(\n",
    "        image_uri=image_uri,\n",
    "        model_data=model_uri,\n",
    "        role=role,\n",
    "        name=config['model_name'],\n",
    "        source_dir='source',\n",
    "        entry_point='inference.py'\n",
    "    )\n",
    "    \n",
    "    # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ—ãƒ­ã‚¤\n",
    "    predictor = model.deploy(\n",
    "        initial_instance_count=config['initial_instance_count'],\n",
    "        instance_type=config['instance_type'],\n",
    "        endpoint_name=config['endpoint_name'],\n",
    "        data_capture_config=data_capture_config,\n",
    "        serializer=JSONSerializer(),\n",
    "        deserializer=JSONDeserializer()\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Model deployed successfully to endpoint: {config['endpoint_name']}\")\n",
    "    return predictor\n",
    "\n",
    "# ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ—ãƒ­ã‚¤å®Ÿè¡Œ\n",
    "predictor = deploy_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ†ã‚¹ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_endpoint(n_samples=10):\n",
    "    \"\"\"\n",
    "    ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ†ã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ§ª Testing endpoint with {n_samples} samples...\")\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "    test_samples = X_test[:n_samples].tolist()\n",
    "    \n",
    "    try:\n",
    "        # äºˆæ¸¬å®Ÿè¡Œ\n",
    "        response = predictor.predict({\"instances\": test_samples})\n",
    "        \n",
    "        print(\"âœ… Endpoint test successful!\")\n",
    "        print(f\"Sample response: {response}\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Endpoint test failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\n",
    "test_response = test_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Monitor ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    \"\"\"\n",
    "    Model Monitorç”¨ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµ±è¨ˆã‚’ä½œæˆ\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š Creating baseline statistics for Model Monitor...\")\n",
    "    \n",
    "    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã‚’ä½¿ç”¨ï¼‰\n",
    "    baseline_size = min(1000, len(X_train))\n",
    "    baseline_indices = np.random.choice(len(X_train), baseline_size, replace=False)\n",
    "    baseline_X = X_train[baseline_indices]\n",
    "    \n",
    "    # ç‰¹å¾´é‡åã‚’ç”Ÿæˆ\n",
    "    feature_names = [f'feature_{i}' for i in range(baseline_X.shape[1])]\n",
    "    \n",
    "    # DataFrameã«å¤‰æ›\n",
    "    baseline_df = pd.DataFrame(baseline_X, columns=feature_names)\n",
    "    \n",
    "    # ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜\n",
    "    os.makedirs('baseline_data', exist_ok=True)\n",
    "    baseline_path = 'baseline_data/baseline.csv'\n",
    "    baseline_df.to_csv(baseline_path, index=False)\n",
    "    \n",
    "    # S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "    s3_baseline_data_uri = f'{s3_baseline_path}/baseline.csv'\n",
    "    session.upload_data(\n",
    "        baseline_path, \n",
    "        bucket=bucket,\n",
    "        key_prefix=f'{config[\"s3_prefix\"]}/{config[\"baseline_path\"]}'\n",
    "    )\n",
    "    \n",
    "    # DefaultModelMonitoråˆæœŸåŒ–\n",
    "    monitor = DefaultModelMonitor(\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        instance_type=config['monitoring_instance_type'],\n",
    "        volume_size_in_gb=20,\n",
    "        max_runtime_in_seconds=config['max_runtime_seconds']\n",
    "    )\n",
    "    \n",
    "    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµ±è¨ˆä½œæˆ\n",
    "    baseline_job_name = f'baseline-job-{int(time.time())}'\n",
    "    \n",
    "    monitor.suggest_baseline(\n",
    "        baseline_dataset=s3_baseline_data_uri,\n",
    "        dataset_format=DatasetFormat.csv(header=True),\n",
    "        output_s3_uri=f'{s3_baseline_path}/output',\n",
    "        job_name=baseline_job_name\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Baseline creation job started: {baseline_job_name}\")\n",
    "    print(f\"Baseline data uploaded to: {s3_baseline_data_uri}\")\n",
    "    \n",
    "    return monitor, s3_baseline_data_uri\n",
    "\n",
    "# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä½œæˆå®Ÿè¡Œ\n",
    "monitor, baseline_uri = create_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Monitoring ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_monitoring():\n",
    "    \"\"\"\n",
    "    Model Monitoringã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¨­å®š\n",
    "    \"\"\"\n",
    "    print(\"â° Setting up Model Monitor schedule...\")\n",
    "    \n",
    "    try:\n",
    "        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å®Œäº†ã‚’å¾…ã¤\n",
    "        print(\"Waiting for baseline job to complete...\")\n",
    "        monitor.latest_baselining_job.wait(logs=False)\n",
    "        \n",
    "        # ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆ\n",
    "        monitor.create_monitoring_schedule(\n",
    "            monitor_schedule_name=config['monitor_schedule_name'],\n",
    "            endpoint_input=EndpointInput(\n",
    "                endpoint_name=config['endpoint_name'],\n",
    "                destination=f'{s3_monitoring_path}/input'\n",
    "            ),\n",
    "            output_s3_uri=f'{s3_monitoring_path}/output',\n",
    "            statistics=monitor.baseline_statistics(),\n",
    "            constraints=monitor.suggested_constraints(),\n",
    "            schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "            enable_cloudwatch_metrics=True\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Monitoring schedule created: {config['monitor_schedule_name']}\")\n",
    "        print(f\"Monitoring reports will be saved to: {s3_monitoring_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to create monitoring schedule: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨­å®šå®Ÿè¡Œ\n",
    "setup_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ãƒ†ã‚¹ãƒˆãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_traffic(n_requests=50, interval=2):\n",
    "    \"\"\"\n",
    "    ãƒ†ã‚¹ãƒˆãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’ç”Ÿæˆã—ã¦Data Captureã‚’ä¿ƒé€²\n",
    "    \"\"\"\n",
    "    print(f\"ğŸš¦ Generating {n_requests} test requests...\")\n",
    "    \n",
    "    responses = []\n",
    "    \n",
    "    for i in range(n_requests):\n",
    "        try:\n",
    "            # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«é¸æŠ\n",
    "            sample_idx = np.random.randint(0, len(X_test))\n",
    "            test_sample = X_test[sample_idx:sample_idx+1].tolist()\n",
    "            \n",
    "            # äºˆæ¸¬å®Ÿè¡Œ\n",
    "            response = predictor.predict({\"instances\": test_sample})\n",
    "            responses.append(response)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Sent {i + 1}/{n_requests} requests\")\n",
    "            \n",
    "            time.sleep(interval)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in request {i + 1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"âœ… Traffic generation completed. {len(responses)} successful requests\")\n",
    "    return responses\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ç”Ÿæˆ\n",
    "traffic_responses = generate_traffic(n_requests=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°çŠ¶æ…‹ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_monitoring_status():\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®çŠ¶æ…‹ã‚’ç¢ºèª\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” Checking monitoring status...\")\n",
    "    \n",
    "    try:\n",
    "        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«çŠ¶æ…‹ç¢ºèª\n",
    "        schedule_desc = monitor.describe_schedule()\n",
    "        print(f\"Monitoring schedule status: {schedule_desc['MonitoringScheduleStatus']}\")\n",
    "        \n",
    "        # æœ€æ–°ã®å®Ÿè¡ŒçŠ¶æ…‹ç¢ºèª\n",
    "        executions = monitor.list_executions()\n",
    "        if executions:\n",
    "            latest_execution = executions[0]\n",
    "            print(f\"Latest execution status: {latest_execution['ProcessingJobStatus']}\")\n",
    "            print(f\"Latest execution time: {latest_execution['CreationTime']}\")\n",
    "        else:\n",
    "            print(\"No executions found yet\")\n",
    "        \n",
    "        return schedule_desc\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking monitoring status: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°çŠ¶æ…‹ç¢ºèª\n",
    "monitoring_status = check_monitoring_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Data Captureç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_capture():\n",
    "    \"\"\"\n",
    "    Data Captureãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“ Checking data capture files...\")\n",
    "    \n",
    "    try:\n",
    "        # S3ã‹ã‚‰Data Captureãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—\n",
    "        response = s3_client.list_objects_v2(\n",
    "            Bucket=bucket,\n",
    "            Prefix=f'{config[\"s3_prefix\"]}/{config[\"data_capture_path\"]}'\n",
    "        )\n",
    "        \n",
    "        if 'Contents' in response:\n",
    "            files = response['Contents']\n",
    "            print(f\"Found {len(files)} data capture files:\")\n",
    "            \n",
    "            for file in files[:5]:  # æœ€åˆã®5ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤º\n",
    "                print(f\"  - {file['Key']} ({file['Size']} bytes)\")\n",
    "                \n",
    "            if len(files) > 5:\n",
    "                print(f\"  ... and {len(files) - 5} more files\")\n",
    "        else:\n",
    "            print(\"No data capture files found yet\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking data capture: {str(e)}\")\n",
    "\n",
    "# Data Captureç¢ºèª\n",
    "check_data_capture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. çµæœã‚µãƒãƒªãƒ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ LIGHTGBM BINARY CLASSIFIER V2 DEPLOYMENT COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ“Š Model Performance:\")\n",
    "print(f\"  - Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"  - F1-Score: {metrics['f1_score']:.4f}\")\n",
    "print(f\"  - AUC: {metrics['auc']:.4f}\")\n",
    "print(f\"\\nğŸš€ Deployment Details:\")\n",
    "print(f\"  - Endpoint: {config['endpoint_name']}\")\n",
    "print(f\"  - Monitor Schedule: {config['monitor_schedule_name']}\")\n",
    "print(f\"  - Instance Type: {config['instance_type']}\")\n",
    "print(f\"\\nğŸ“ S3 Resources:\")\n",
    "print(f\"  - Bucket: {bucket}\")\n",
    "print(f\"  - Model: {s3_model_path}\")\n",
    "print(f\"  - Baseline: {s3_baseline_path}\")\n",
    "print(f\"  - Monitoring: {s3_monitoring_path}\")\n",
    "print(f\"  - Data Capture: {s3_data_capture_path}\")\n",
    "print(f\"\\nâš ï¸  IMPORTANT: Remember to clean up resources to avoid charges!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå¿…è¦æ™‚ã«å®Ÿè¡Œï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    \"\"\"\n",
    "    ãƒªã‚½ãƒ¼ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§¹ Starting cleanup process...\")\n",
    "    \n",
    "    try:\n",
    "        # ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«åœæ­¢ãƒ»å‰Šé™¤\n",
    "        try:\n",
    "            monitor.stop_monitoring_schedule()\n",
    "            print(\"âœ… Monitoring schedule stopped\")\n",
    "            \n",
    "            time.sleep(10)  # åœæ­¢ã‚’å¾…ã¤\n",
    "            \n",
    "            monitor.delete_monitoring_schedule()\n",
    "            print(\"âœ… Monitoring schedule deleted\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Error cleaning up monitor: {str(e)}\")\n",
    "        \n",
    "        # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå‰Šé™¤\n",
    "        try:\n",
    "            predictor.delete_endpoint()\n",
    "            print(\"âœ… Endpoint deleted\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Error deleting endpoint: {str(e)}\")\n",
    "        \n",
    "        print(\"âœ… Cleanup completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during cleanup: {str(e)}\")\n",
    "\n",
    "# ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã—ã¦å®Ÿè¡Œï¼‰\n",
    "# cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
