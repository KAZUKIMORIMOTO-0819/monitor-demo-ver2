{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Binary Classification + SageMaker Model Monitor v2\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€LightGBMã‚’ä½¿ã£ãŸ2å€¤åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’SageMakerã«ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã€Model Monitorã§ç›£è¦–ã™ã‚‹å®Œå…¨ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè£…ã—ã¾ã™ã€‚\n",
    "\n",
    "## ä¸»ãªæ”¹è‰¯ç‚¹\n",
    "- ã‚ˆã‚Šå …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°\n",
    "- è©³ç´°ãªãƒ­ã‚°å‡ºåŠ›\n",
    "- è¨­å®šå¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "- è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ©Ÿèƒ½\n",
    "- åŒ…æ‹¬çš„ãªãƒ†ã‚¹ãƒˆæ©Ÿèƒ½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­å®šã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install lightgbm>=3.3.0 -q\n",
    "!pip install sagemaker>=2.100.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import tarfile\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# SageMakeré–¢é€£\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.model_monitor import (\n",
    "    DataCaptureConfig,\n",
    "    DefaultModelMonitor,\n",
    "    DatasetFormat,\n",
    "    EndpointInput,\n",
    "    CronExpressionGenerator\n",
    ")\n",
    "\n",
    "# è­¦å‘Šã‚’æŠ‘åˆ¶\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ãƒ­ã‚°è¨­å®š\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åˆæœŸè¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMakerè¨­å®š\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = session.boto_region_name\n",
    "bucket = session.default_bucket()\n",
    "s3_client = boto3.client('s3')\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Bucket: {bucket}\")\n",
    "print(f\"Role: {role}\")\n",
    "\n",
    "# è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "config = {\n",
    "    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆè¨­å®š\n",
    "    'n_samples': 3000,\n",
    "    'n_features': 25,\n",
    "    'n_informative': 20,\n",
    "    'n_redundant': 5,\n",
    "    'n_clusters_per_class': 2,\n",
    "    'test_size': 0.2,\n",
    "    'val_size': 0.1,\n",
    "    'random_state': 42,\n",
    "    \n",
    "    # LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    'lgb_params': {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42\n",
    "    },\n",
    "    'num_boost_round': 100,\n",
    "    'early_stopping_rounds': 10,\n",
    "    \n",
    "    # ãƒ‡ãƒ—ãƒ­ã‚¤è¨­å®š\n",
    "    'model_name': f'lightgbm-binary-v2-{int(time.time())}',\n",
    "    'endpoint_name': f'lightgbm-binary-endpoint-v2-{int(time.time())}',\n",
    "    'instance_type': 'ml.m5.large',\n",
    "    'initial_instance_count': 1,\n",
    "    \n",
    "    # Data Captureè¨­å®š\n",
    "    'enable_data_capture': True,\n",
    "    'sampling_percentage': 100,\n",
    "    'capture_modes': ['Input', 'Output'],\n",
    "    \n",
    "    # Model Monitorè¨­å®š\n",
    "    'monitor_schedule_name': f'lightgbm-monitor-v2-{int(time.time())}',\n",
    "    'monitoring_instance_type': 'ml.m5.xlarge',\n",
    "    'monitoring_cron': 'cron(0/10 * ? * * *)',  # 10åˆ†æ¯\n",
    "    'max_runtime_seconds': 3600,\n",
    "    \n",
    "    # S3è¨­å®š\n",
    "    's3_prefix': 'lightgbm-binary-classification-v2',\n",
    "    'model_artifacts_path': 'model',\n",
    "    'baseline_path': 'baseline',\n",
    "    'monitoring_reports_path': 'monitoring-reports',\n",
    "    'data_capture_path': 'datacapture'\n",
    "}\n",
    "\n",
    "# S3ãƒ‘ã‚¹è¨­å®š\n",
    "s3_model_path = f's3://{bucket}/{config[\"s3_prefix\"]}/{config[\"model_artifacts_path\"]}'\n",
    "s3_baseline_path = f's3://{bucket}/{config[\"s3_prefix\"]}/{config[\"baseline_path\"]}'\n",
    "s3_monitoring_path = f's3://{bucket}/{config[\"s3_prefix\"]}/{config[\"monitoring_reports_path\"]}'\n",
    "s3_data_capture_path = f's3://{bucket}/{config[\"s3_prefix\"]}/{config[\"data_capture_path\"]}'\n",
    "\n",
    "print(f\"\\nğŸ“ S3 Paths:\")\n",
    "print(f\"Model: {s3_model_path}\")\n",
    "print(f\"Baseline: {s3_baseline_path}\")\n",
    "print(f\"Monitoring: {s3_monitoring_path}\")\n",
    "print(f\"Data Capture: {s3_data_capture_path}\")\n",
    "\n",
    "print(f\"\\nğŸš€ Configuration:\")\n",
    "print(f\"Endpoint: {config['endpoint_name']}\")\n",
    "print(f\"Monitor: {config['monitor_schedule_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¨å‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    \"\"\"\n",
    "    2å€¤åˆ†é¡ç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š Generating sample data for binary classification...\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\n",
    "    X, y = make_classification(\n",
    "        n_samples=config['n_samples'],\n",
    "        n_features=config['n_features'],\n",
    "        n_informative=config['n_informative'],\n",
    "        n_redundant=config['n_redundant'],\n",
    "        n_clusters_per_class=config['n_clusters_per_class'],\n",
    "        random_state=config['random_state']\n",
    "    )\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=config['test_size'], \n",
    "        random_state=config['random_state'], stratify=y\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=config['val_size']/(1-config['test_size']), \n",
    "        random_state=config['random_state'], stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Data generated successfully:\")\n",
    "    print(f\"  Train: {X_train.shape[0]} samples\")\n",
    "    print(f\"  Validation: {X_val.shape[0]} samples\")\n",
    "    print(f\"  Test: {X_test.shape[0]} samples\")\n",
    "    print(f\"  Features: {X_train.shape[1]}\")\n",
    "    print(f\"  Class distribution (train): {np.bincount(y_train)}\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Ÿè¡Œ\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LightGBMãƒ¢ãƒ‡ãƒ«è¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \"\"\"\n",
    "    LightGBMãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¤– Training LightGBM model...\")\n",
    "    \n",
    "    # LightGBMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´\n",
    "    model = lgb.train(\n",
    "        config['lgb_params'],\n",
    "        train_data,\n",
    "        num_boost_round=config['num_boost_round'],\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'val'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(config['early_stopping_rounds']),\n",
    "            lgb.log_evaluation(period=10)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Model training completed successfully\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡\n",
    "    \"\"\"\n",
    "    # äºˆæ¸¬\n",
    "    y_pred_proba = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(\"ğŸ“ˆ Model Performance Metrics:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Ÿè¡Œ\n",
    "model = train_model()\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«è©•ä¾¡\n",
    "metrics = evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã¨æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ã‚’S3ã«ä¿å­˜\n",
    "    \"\"\"\n",
    "    print(\"ğŸ’¾ Saving model artifacts...\")\n",
    "    \n",
    "    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "    os.makedirs('model', exist_ok=True)\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚’pickleå½¢å¼ã§ä¿å­˜ï¼ˆLightGBMã®unordered_map::atã‚¨ãƒ©ãƒ¼å›é¿ï¼‰\n",
    "    with open('model/model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ã‚‚ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰\n",
    "    model.save_model('model/model.txt')\n",
    "    \n",
    "    # tar.gzå½¢å¼ã§ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–\n",
    "    with tarfile.open('model.tar.gz', 'w:gz') as tar:\n",
    "        tar.add('model', arcname='.')\n",
    "    \n",
    "    # S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "    model_uri = f'{s3_model_path}/model.tar.gz'\n",
    "    session.upload_data('model.tar.gz', bucket=bucket, \n",
    "                        key_prefix=f'{config[\"s3_prefix\"]}/{config[\"model_artifacts_path\"]}')\n",
    "    \n",
    "    print(f\"âœ… Model saved to: {model_uri}\")\n",
    "    return model_uri\n",
    "\n",
    "def create_inference_script():\n",
    "    \"\"\"\n",
    "    æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“ Creating inference script...\")\n",
    "    \n",
    "    os.makedirs('source', exist_ok=True)\n",
    "    \n",
    "    inference_code = '''import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\"\"\"\n",
    "    try:\n",
    "        with open(f\"{model_dir}/model.pkl\", \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "        logger.info(\"Model loaded successfully from pickle file\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†\"\"\"\n",
    "    if request_content_type == \"application/json\":\n",
    "        try:\n",
    "            input_data = json.loads(request_body)\n",
    "            \n",
    "            # å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«ã®å ´åˆ\n",
    "            if \"instances\" in input_data:\n",
    "                data = np.array(input_data[\"instances\"])\n",
    "            elif isinstance(input_data, list):\n",
    "                data = np.array(input_data)\n",
    "            else:\n",
    "                data = np.array([input_data])\n",
    "                \n",
    "            logger.info(f\"Input data shape: {data.shape}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing input: {str(e)}\")\n",
    "            raise ValueError(f\"Invalid input format: {str(e)}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"äºˆæ¸¬ã‚’å®Ÿè¡Œ\"\"\"\n",
    "    try:\n",
    "        # ç¢ºç‡äºˆæ¸¬\n",
    "        probabilities = model.predict(input_data, num_iteration=model.best_iteration)\n",
    "        \n",
    "        # 2å€¤åˆ†é¡ã®å ´åˆã€ã‚¯ãƒ©ã‚¹1ã®ç¢ºç‡ã‚’è¿”ã™\n",
    "        if len(probabilities.shape) == 1:\n",
    "            predictions = probabilities\n",
    "        else:\n",
    "            predictions = probabilities[:, 1]\n",
    "            \n",
    "        # é–¾å€¤0.5ã§ã‚¯ãƒ©ã‚¹äºˆæ¸¬\n",
    "        classes = (predictions > 0.5).astype(int)\n",
    "        \n",
    "        logger.info(f\"Predictions generated for {len(predictions)} samples\")\n",
    "        \n",
    "        return {\n",
    "            \"predictions\": classes.tolist(),\n",
    "            \"probabilities\": predictions.tolist()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during prediction: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    \"\"\"å‡ºåŠ›ã‚’å‡¦ç†\"\"\"\n",
    "    if content_type == \"application/json\":\n",
    "        return json.dumps(prediction)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {content_type}\")\n",
    "'''\n",
    "    \n",
    "    with open('source/inference.py', 'w') as f:\n",
    "        f.write(inference_code)\n",
    "    \n",
    "    # requirements.txtä½œæˆ\n",
    "    requirements = '''lightgbm>=3.3.0\n",
    "numpy>=1.21.0\n",
    "scikit-learn>=1.0.0\n",
    "'''\n",
    "    \n",
    "    with open('source/requirements.txt', 'w') as f:\n",
    "        f.write(requirements)\n",
    "    \n",
    "    print(\"âœ… Inference script created successfully\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã¨æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ\n",
    "model_uri = save_model(model)\n",
    "create_inference_script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
